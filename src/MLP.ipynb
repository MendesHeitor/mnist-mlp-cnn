{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpcMwplW7yO9"
      },
      "source": [
        "## Importing libraries\n",
        "\n",
        "This MLP network is created from scratch using only the numpy library. It focuses on introductory algorithms and techniques, making numpy the sole necessary library. This minimalist approach highlights the fundamental concepts of neural networks while relying on numpy's powerful numerical computations for essential operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "S5t-ggY87spL"
      },
      "outputs": [],
      "source": [
        "import mnist_loader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQdF5mNWAzo2"
      },
      "source": [
        "## Activation Functions Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "r0ePfHTqBEX3"
      },
      "outputs": [],
      "source": [
        "class Activation:\n",
        "  \"\"\"\n",
        "    - Activation functions.\n",
        "    - And their derivatives\n",
        "  \"\"\"\n",
        "\n",
        "  @classmethod\n",
        "  def sigmoid(cls, vec):\n",
        "    \"\"\"\n",
        "      Sigmoid activation function.\n",
        "    \"\"\"\n",
        "    return 1.0 / (1.0 + np.exp(-vec))\n",
        "\n",
        "  @classmethod\n",
        "  def sigmoid_drv(cls, vec):\n",
        "    \"\"\"\n",
        "      Sigmoid function derivative.\n",
        "    \"\"\"\n",
        "    return cls.sigmoid(vec) * (1 - cls.sigmoid(vec))\n",
        "\n",
        "  @classmethod\n",
        "  def step(cls, vec):\n",
        "    \"\"\"\n",
        "      Binary Step activation function.\n",
        "    \"\"\"\n",
        "    return np.where(vec > 0, 1, 0)\n",
        "\n",
        "  @classmethod\n",
        "  def relu(cls, vec):\n",
        "    \"\"\"\n",
        "      Rectified Linear Unit\n",
        "    \"\"\"\n",
        "    return np.maximum(0, vec)\n",
        "\n",
        "  @classmethod\n",
        "  def relu_drv(cls, vec):\n",
        "    \"\"\"\n",
        "      Rectified Linear Unit Derivative\n",
        "    \"\"\"\n",
        "    return np.where(vec > 0, 1, 0)\n",
        "\n",
        "  @classmethod\n",
        "  def tanh(cls, vec):\n",
        "    \"\"\"\n",
        "      Hiperbolic Tangent\n",
        "    \"\"\"\n",
        "    return np.tanh(vec)\n",
        "\n",
        "  @classmethod\n",
        "  def tanh_drv(cls, vec):\n",
        "    \"\"\"\n",
        "      Hiperbolic Tangent Derivative\n",
        "    \"\"\"\n",
        "    return 1 - np.tanh(vec)**2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVkNFNNSA98-"
      },
      "source": [
        "## Loss Function Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vBoe8OBxBSRn"
      },
      "outputs": [],
      "source": [
        "class Cost:\n",
        "  \"\"\"\n",
        "    - Cost functions.\n",
        "  \"\"\"\n",
        "\n",
        "  @classmethod\n",
        "  def cost_derivative(cls, output_activations, y):\n",
        "    \"\"\"\n",
        "      - Return the vector of partial derivatives partial C(x).\n",
        "      - Partial a for the output activations.\n",
        "    \"\"\"\n",
        "    return (output_activations-y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-IOM4EH9V5Z"
      },
      "source": [
        "## Network Object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "kwDEbCL_9OIc"
      },
      "outputs": [],
      "source": [
        "class Network:\n",
        "  \"\"\"\n",
        "    - A Feed Foward Neural Network.\n",
        "    - Uses Stochastic Gradient Descent learning algorithm.\n",
        "    - Gradients are being caculated using back propagation.\n",
        "    - Misses some optimizations and omits some desirable features.\n",
        "  \"\"\"\n",
        "  def __init__(self, sizes, activation_function = Activation.sigmoid, actv_drv = Activation.sigmoid_drv):\n",
        "    self.num_layers = len(sizes)\n",
        "    self.sizes = sizes\n",
        "    self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "    self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "    self.activation_function = activation_function\n",
        "    self.actv_drv = actv_drv\n",
        "\n",
        "  def feed_foward(self, a):\n",
        "    \"\"\"\n",
        "      Calculates the final result of a input going through the NN.\n",
        "    \"\"\"\n",
        "    for b, w in zip(self.biases, self.weights):\n",
        "      a = self.activation_function(np.dot(w, a) + b)\n",
        "    return a\n",
        "\n",
        "  def update_mini_batch(self,  mini_batch, eta):\n",
        "    \"\"\"\n",
        "      - Updates the network's weights and biases by applying gradient descent using backpropagation to a single mini batch.\n",
        "      - The \"mini_batch\" is a list of tuples \"(x, y)\".\n",
        "      - \"eta is the learning rate.\n",
        "    \"\"\"\n",
        "\n",
        "    nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "    nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "    # Nabla lists are lists of numpy arrays, in the same way as self.weights and self.biases\n",
        "    # They store, by adding, all the changes in the weights calculated by the back propagation algorithm\n",
        "    for x, y in mini_batch:\n",
        "      delta_nabla_b, delta_nabla_w = self.back_propagation(x, y)\n",
        "\n",
        "      nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "      nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "\n",
        "    # Recalculate weights for a mini-batch run\n",
        "    self.weights = [w - (eta/len(mini_batch)) * nw for w, nw in zip(self.weights, nabla_w)]\n",
        "    self.biases = [b - (eta/len(mini_batch)) * nb for b, nb in zip(self.biases, nabla_b)]\n",
        "\n",
        "  def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
        "    \"\"\"\n",
        "      - Utilizes mini-batch stochastic gradient descent for training the neural network.\n",
        "      - The \"training_data\" consists of tuples \"(x, y)\" representing input and desired output.\n",
        "      - Other parameters are self-explanatory.\n",
        "      - If \"test_data\" is provided, the network evaluates against it after each epoch, allowing for progress tracking but at a slower pace.\n",
        "    \"\"\"\n",
        "\n",
        "    if test_data:\n",
        "      test_data = list(test_data)\n",
        "      n_test = len(test_data)\n",
        "\n",
        "    training_data = list(training_data)\n",
        "    n = len(training_data)\n",
        "\n",
        "    for j in range(epochs):\n",
        "      random.shuffle(training_data)\n",
        "      mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
        "\n",
        "      for mini_batch in mini_batches:\n",
        "        self.update_mini_batch(mini_batch, eta)\n",
        "\n",
        "      if test_data :\n",
        "        print(f\"Epoch {j}: {self.evaluate(test_data)} / {n_test}\")\n",
        "\n",
        "      else:\n",
        "        print(f\"Epoch {j}: complete\")\n",
        "\n",
        "\n",
        "  def back_propagation(self, x, y):\n",
        "    \"\"\"\n",
        "      - Returns a tuple \"(nabla_b, nabla_w)\" representing the gradient for the chosen Cost Function C(x).\n",
        "      - \"nabla_b\" & \"nabla_w\" are layer-by-layer lists of numpy arrays.\n",
        "      - Similar to \"self.biases\" and \"self.weights\".\n",
        "      - How much is the adjustment.\n",
        "    \"\"\"\n",
        "\n",
        "    nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "    nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "    # Feed-foward phase\n",
        "\n",
        "    activation = x\n",
        "    activations = [x] # Stores activations layer by layer\n",
        "    zs = [] # Stores all the z vectores, layer-by-layer\n",
        "\n",
        "    for b, w in zip(self.biases, self.weights):\n",
        "      z = np.dot(w, activation) + b\n",
        "      zs.append(z)\n",
        "      activation = self.activation_function(z)\n",
        "      activations.append(activation)\n",
        "\n",
        "    # Backward pass -> delta rule\n",
        "    delta = Cost.cost_derivative(activations[-1], y) * self.actv_drv(zs[-1])\n",
        "\n",
        "    nabla_b[-1] = delta\n",
        "    nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "\n",
        "    # In the following loop we want to iterate from the last to the first layer, excluding the output and the input layers\n",
        "    # We are taking advantage that python can use negative indices\n",
        "\n",
        "    for l in range(2, self.num_layers):\n",
        "      z = zs[-l]\n",
        "      sig_dv = self.actv_drv(z)\n",
        "      delta = np.dot(self.weights[-l+1].transpose(), delta) * sig_dv\n",
        "      nabla_b[-l] = delta\n",
        "      nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "\n",
        "    return (nabla_b, nabla_w)\n",
        "\n",
        "\n",
        "  def evaluate(self, test_data):\n",
        "    \"\"\"\n",
        "      - Evaluates the number of correct predictions of the NN.\n",
        "      - The output is chosen by the \"winner takes all\" rule, where the highest number on the output vector is the prediction\n",
        "    \"\"\"\n",
        "    test_results = [(np.argmax(self.feed_foward(x)), y) for (x, y) in test_data]\n",
        "\n",
        "    return sum (int(x == y) for (x, y) in test_results)\n",
        "\n",
        "\n",
        "  def debug(self):\n",
        "    \"\"\"\n",
        "      - Write any code for debbuging.\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpnxVCCqnt30"
      },
      "source": [
        "## Training\n",
        "\n",
        "In this section we're going to train this simple Mlp using the mnist dataset in the \".pkl\" file, using its default data division."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPmcgIWWO4SQ"
      },
      "source": [
        "## Tuning\n",
        "\n",
        "Now, we're going to explore the hyperparameters of this MLP, in order to find the most adequate configuration for it."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activation Function"
      ],
      "metadata": {
        "id": "jXDDT-VFaRcU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tanh Function\n",
        "\n",
        "\n",
        "The hyperbolic tangent (tanh) function didn't give us good results. We used the same parameters as before when we were using the sigmoid function, but the tanh function didn't work well in this case. Initially, the accuracy improved in the first 5-7 epochs, but after that, the performance of the network either stayed the same or got worse. Overall, the tanh function didn't perform well."
      ],
      "metadata": {
        "id": "-E_OupMkaSXI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1o3xywvNnslZ",
        "outputId": "41e8398f-c0e7-46cd-ea7f-d1dd2eb0d8a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: 1282 / 10000\n",
            "Epoch 1: 1387 / 10000\n",
            "Epoch 2: 1184 / 10000\n",
            "Epoch 3: 965 / 10000\n",
            "Epoch 4: 1041 / 10000\n",
            "Epoch 5: 1715 / 10000\n",
            "Epoch 6: 1670 / 10000\n",
            "Epoch 7: 1670 / 10000\n",
            "Epoch 8: 1670 / 10000\n",
            "Epoch 9: 1670 / 10000\n",
            "Epoch 10: 1670 / 10000\n",
            "Epoch 11: 1670 / 10000\n",
            "Epoch 12: 1670 / 10000\n",
            "Epoch 13: 1670 / 10000\n",
            "Epoch 14: 1670 / 10000\n",
            "Epoch 15: 1670 / 10000\n",
            "Epoch 16: 1670 / 10000\n",
            "Epoch 17: 1670 / 10000\n",
            "Epoch 18: 1670 / 10000\n",
            "Epoch 19: 1610 / 10000\n",
            "Epoch 20: 1373 / 10000\n",
            "Epoch 21: 1368 / 10000\n",
            "Epoch 22: 1375 / 10000\n",
            "Epoch 23: 1370 / 10000\n",
            "Epoch 24: 1370 / 10000\n",
            "Epoch 25: 1370 / 10000\n",
            "Epoch 26: 1370 / 10000\n",
            "Epoch 27: 1369 / 10000\n",
            "Epoch 28: 1369 / 10000\n",
            "Epoch 29: 1368 / 10000\n"
          ]
        }
      ],
      "source": [
        "net = Network([784, 30, 10], Activation.tanh, Activation.tanh_drv)\n",
        "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
        "\n",
        "#epochs, mini_batch_size, eta,\n",
        "net.SGD(training_data, 30, 10, 4.0, test_data=test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ETA - learning rate"
      ],
      "metadata": {
        "id": "TyCo5utgaURK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mini batch size"
      ],
      "metadata": {
        "id": "Nm5q60_KaWIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Number of Epochs"
      ],
      "metadata": {
        "id": "rAc3XjDxaebB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The network architecture"
      ],
      "metadata": {
        "id": "8w1Z7Hpwag7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regularization / Droup-out"
      ],
      "metadata": {
        "id": "NYm1BbZeaisi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Machine Learning Algorithm"
      ],
      "metadata": {
        "id": "vTeITtZBalTF"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}